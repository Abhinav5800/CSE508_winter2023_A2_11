{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiFHfakUA_1Z",
        "outputId": "08295315-cc49-4a9e-b8d1-44b25cd3d033"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP0XFKcJAeSI",
        "outputId": "df7aa468-5c7a-4b1a-ddb8-42ac0a0134f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ArticleId                                               Text  Category\n",
            "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
            "1        154  german business confidence slides german busin...  business\n",
            "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
            "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
            "4        917  enron bosses in $168m payout eighteen former e...  business\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('BBC News Train.csv')\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm83kM-oNjyu",
        "outputId": "c23e3aed-5d6a-44d4-d3d7-0d3d32cb9bcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1490, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(string):\n",
        "    return string.lower()\n",
        "def remove_punc(string):\n",
        "    txt = re.sub(r'[^\\w\\s]', '', string)\n",
        "    return txt\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "def remove_stopwords(data):\n",
        "    words = word_tokenize(str(data))\n",
        "    res = ' '.join([word for word in words if word not in cachedStopWords])\n",
        "    return np.char.strip(res)\n",
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
        "    return np.char.strip(new_text)\n",
        "def remove_punc(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    return data"
      ],
      "metadata": {
        "id": "g-a0uNbeA_4V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(d):\n",
        "    no_punc = []\n",
        "    d_lower=d.lower()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    nltk_tokens = nltk.word_tokenize(d_lower)\n",
        "    nltk_tokens = remove_punc(nltk_tokens)\n",
        "    \n",
        "    stop_words_removed = []\n",
        "    for w in nltk_tokens:\n",
        "        if w not in cachedStopWords:\n",
        "            stop_words_removed.append(w)\n",
        "    new_words = []\n",
        "    for x in stop_words_removed:\n",
        "        if(x.isalnum() and x!=\" \"):\n",
        "            new_words.append(x)\n",
        "    new_text = []\n",
        "    for w in new_words:\n",
        "        new_text.append(lemmatizer.lemmatize(w))\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "m9o4jn8JD8ei"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data)):\n",
        "    text = data['Text'][i]\n",
        "    text = lowercase(text)\n",
        "    text = remove_punc(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatization(text)\n",
        "    data['Text'][i] = text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi2hWRlLCG0p",
        "outputId": "7e2925f2-18d7-4653-9335-851d2d794c12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a27b383f40fb>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Text'][i] = text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVkDzYymCG3n",
        "outputId": "f3d2efe2-ac78-4851-c634-6f696c3b060f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ArticleId                                               Text  Category\n",
            "0       1833  worldcom ex bos launch defence lawyer defendin...  business\n",
            "1        154  german business confidence slide german busine...  business\n",
            "2       1101  bbc poll indicates economic gloom citizen majo...  business\n",
            "3       1976  lifestyle governs mobile choice faster better ...      tech\n",
            "4        917  enron boss 168m payout eighteen former enron d...  business\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "business_class = []\n",
        "entertainment_class = []\n",
        "politics_class = []\n",
        "sport_class = []\n",
        "tech_class  =[]\n",
        "\n",
        "for i in range(len(data)):\n",
        "  if(data['Category'][i]==\"business\"):\n",
        "    business_class.append(data['Text'][i])\n",
        "  if(data['Category'][i]==(\"entertainment\")):\n",
        "    entertainment_class.append(data['Text'][i])\n",
        "  if(data['Category'][i]==(\"politics\")):\n",
        "    politics_class.append(data['Text'][i])\n",
        "  if(data['Category'][i]==(\"sport\")):\n",
        "    sport_class.append(data['Text'][i])\n",
        "  if(data['Category'][i]==(\"tech\")):\n",
        "    tech_class.append(data['Text'][i])\n"
      ],
      "metadata": {
        "id": "ZmdxYhhTCG5_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bcl_pre = []\n",
        "ecl_pre = []\n",
        "pcl_pre = []\n",
        "scl_pre = []\n",
        "tcl_pre = []\n",
        "for f in business_class:\n",
        "  bcl_pre.append(preprocessing(str(f)))\n",
        "for f in entertainment_class:\n",
        "  ecl_pre.append(preprocessing(str(f)))\n",
        "for f in politics_class:\n",
        "  pcl_pre.append(preprocessing(str(f)))\n",
        "for f in sport_class:\n",
        "  scl_pre.append(preprocessing(str(f)))\n",
        "for f in tech_class:\n",
        "  tcl_pre.append(preprocessing(str(f)))"
      ],
      "metadata": {
        "id": "VuqpSQA4EaRR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "all_data.append(bcl_pre)\n",
        "all_data.append(ecl_pre)\n",
        "all_data.append(pcl_pre)\n",
        "all_data.append(scl_pre)\n",
        "all_data.append(tcl_pre)"
      ],
      "metadata": {
        "id": "MSJjbNlREaU3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_icf_list(train_test_lists):\n",
        "    tf_list_of_dict = []\n",
        "    for c in range(5):\n",
        "        tf_list_of_dict.append({})\n",
        "    cf_dict = {}\n",
        "    icf_dict = {}\n",
        "    for c in range(5):\n",
        "        for doc_terms in train_test_lists[c][0]:\n",
        "            for term in doc_terms:\n",
        "                if(term in tf_list_of_dict[c]):\n",
        "                    tf_list_of_dict[c][term]+=1\n",
        "                else:\n",
        "                    tf_list_of_dict[c][term]=1\n",
        "    \n",
        "    for tf_dict_class_c in tf_list_of_dict:\n",
        "        for term in tf_dict_class_c:\n",
        "            if(term in cf_dict):\n",
        "                cf_dict[term]+=1\n",
        "            else:\n",
        "                cf_dict[term]=1\n",
        "    \n",
        "    for term in cf_dict:\n",
        "        icf_dict[term] = math.log2(5/cf_dict[term])\n",
        "    \n",
        "    tf_icf_list_of_dict = []\n",
        "    for c in range(5):\n",
        "        tf_icf_list_of_dict.append({})\n",
        "    for c in range(5):\n",
        "        for term in tf_list_of_dict[c]:\n",
        "            tf_icf_list_of_dict[c][term] = tf_list_of_dict[c][term]*icf_dict[term]\n",
        "    \n",
        "    return tf_list_of_dict, icf_dict, tf_icf_list_of_dict"
      ],
      "metadata": {
        "id": "jzDENhUfTf0z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Q3(train_test_lists, num_of_classes):\n",
        "    # tf-icf\n",
        "    tf_list_of_dict, icf_dict, tf_icf_list_of_dict = get_tf_icf_list(train_test_lists)\n",
        "    list_of_vocab_class_c = []\n",
        "    for c in range(num_of_classes):\n",
        "        vocab_class_c = {}\n",
        "        for tup in tf_icf_list_of_dict[c].items():\n",
        "            vocab_class_c[tup[0]]=tup[1]\n",
        "        list_of_vocab_class_c.append(vocab_class_c)\n",
        "\n",
        "    # global vocabulary\n",
        "    global_vocab = {}\n",
        "    for c in range(num_of_classes):\n",
        "        global_vocab = global_vocab | list_of_vocab_class_c[c].keys()\n",
        "\n",
        "    # calculate priors\n",
        "    num_docs_class_c = []\n",
        "    total_docs = 0\n",
        "    for c in range(num_of_classes):\n",
        "        total_docs+=len(train_test_lists[c][0])\n",
        "        num_docs_class_c.append(len(train_test_lists[c][0]))\n",
        "    prior_class_c = [x/total_docs for x in num_docs_class_c]\n",
        "    \n",
        "    num_terms_class_c_vocab_c = []\n",
        "    for c in range(num_of_classes):\n",
        "        summation=0\n",
        "        for term in global_vocab:\n",
        "            if(term in tf_icf_list_of_dict[c].keys()):\n",
        "                summation+=tf_list_of_dict[c][term]\n",
        "        num_terms_class_c_vocab_c.append(summation)\n",
        "    \n",
        "    # testing\n",
        "    confusion_matrix = np.zeros((num_of_classes,num_of_classes))\n",
        "    total_predictions = 0\n",
        "    for c in range(num_of_classes):\n",
        "        for test_doc_tokens in train_test_lists[c][1]:\n",
        "            posterior_class_c = []\n",
        "            for cc in range(num_of_classes):\n",
        "                posterior_class_c.append(prior_class_c[cc])\n",
        "            for term in test_doc_tokens:\n",
        "                for class_c in range(num_of_classes):\n",
        "                    if(term not in global_vocab):\n",
        "                        continue\n",
        "                    temp = 1.0\n",
        "                    if(term in list_of_vocab_class_c[class_c]):\n",
        "                        temp = (1 + tf_list_of_dict[class_c][term])/(len(global_vocab) + num_terms_class_c_vocab_c[class_c])\n",
        "                    else:\n",
        "                        temp = 1/(len(global_vocab) + num_terms_class_c_vocab_c[class_c])\n",
        "                    posterior_class_c[class_c] = posterior_class_c[class_c]*temp\n",
        "            predicted_class = posterior_class_c.index(max(posterior_class_c))\n",
        "            total_predictions+=1\n",
        "            confusion_matrix[c][predicted_class]+=1\n",
        "    print(confusion_matrix)\n",
        "    \n",
        "    return 100*np.trace(confusion_matrix)/total_predictions,confusion_matrix"
      ],
      "metadata": {
        "id": "KBGqmws2Tf3g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_function(train_frac, list_of_files_class_c):\n",
        "    random.shuffle(list_of_files_class_c)\n",
        "    train_size = int(train_frac*len(list_of_files_class_c))\n",
        "    train_list_class_c = list_of_files_class_c[:train_size]\n",
        "    test_list_class_c = list_of_files_class_c[train_size:]\n",
        "    return train_list_class_c, test_list_class_c"
      ],
      "metadata": {
        "id": "GdG3WhW6YkX-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fractions = [0.7, 0.6, 0.8]\n",
        "for train_frac in train_fractions:\n",
        "        train_test_lists = []\n",
        "        for class_num in range(5):\n",
        "            train_list_class_c, test_list_class_c = train_test_split_function(train_frac, all_data[class_num])\n",
        "            train_test_lists.append([train_list_class_c, test_list_class_c])\n",
        "        print()\n",
        "        accuracy,cm = Q3(train_test_lists, 5)\n",
        "        print('Accuracy for train split fraction ',train_frac, ' is ',accuracy,'%')\n",
        "        precision = sum([cm[i][i] / sum(cm[:,i]) if sum(cm[:,i]) != 0 else 0 for i in range(len(cm))]) / len(cm)\n",
        "        print('Precision for train split fraction ',train_frac, ' is ',precision*100,'%')\n",
        "        recall = sum([cm[i][i] / sum(cm[i,:]) if sum(cm[i,:]) != 0 else 0 for i in range(len(cm))]) / len(cm)\n",
        "        print('Recall for train split fraction ',train_frac, ' is ',recall*100,'%')\n",
        "        f1_score = 2 * precision * recall / (precision + recall)\n",
        "        print('F1_score for train split fraction ',train_frac, ' is ',f1_score*100,'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhf1w3uiTf7G",
        "outputId": "5911b996-a23c-4606-e5c7-18346ea913bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[101.   0.   0.   0.   0.]\n",
            " [ 81.   1.   0.   0.   0.]\n",
            " [ 79.   0.   4.   0.   0.]\n",
            " [ 89.   0.   0.  15.   0.]\n",
            " [ 79.   0.   0.   0.   0.]]\n",
            "Accuracy for train split fraction  0.7  is  26.948775055679288 %\n",
            "Precision for train split fraction  0.7  is  64.70862470862471 %\n",
            "Recall for train split fraction  0.7  is  24.09237324532652 %\n",
            "F1_score for train split fraction  0.7  is  35.11186528512605 %\n",
            "\n",
            "[[135.   0.   0.   0.   0.]\n",
            " [106.   4.   0.   0.   0.]\n",
            " [107.   0.   3.   0.   0.]\n",
            " [120.   0.   0.  19.   0.]\n",
            " [105.   0.   0.   0.   0.]]\n",
            "Accuracy for train split fraction  0.6  is  26.87813021702838 %\n",
            "Precision for train split fraction  0.6  is  64.71204188481676 %\n",
            "Recall for train split fraction  0.6  is  24.006540222367562 %\n",
            "F1_score for train split fraction  0.6  is  35.02112408655337 %\n",
            "\n",
            "[[68.  0.  0.  0.  0.]\n",
            " [54.  1.  0.  0.  0.]\n",
            " [51.  0.  4.  0.  0.]\n",
            " [61.  0.  0.  9.  0.]\n",
            " [53.  0.  0.  0.  0.]]\n",
            "Accuracy for train split fraction  0.8  is  27.242524916943523 %\n",
            "Precision for train split fraction  0.8  is  64.73867595818815 %\n",
            "Recall for train split fraction  0.8  is  24.38961038961039 %\n",
            "F1_score for train split fraction  0.8  is  35.43097592156153 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "training_data, testing_data = train_test_split(data, test_size=0.3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['Category'], test_size=0.3, stratify=data['Category'])"
      ],
      "metadata": {
        "id": "LcBMvr07Zdd8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new = []\n",
        "for i in X_train['Text']:\n",
        "  i = str(i)\n",
        "  X_train_new.append(i)\n",
        "X_test_new = []\n",
        "for i in X_test['Text']:\n",
        "  i = str(i)\n",
        "  X_test_new.append(i)"
      ],
      "metadata": {
        "id": "xiBBMiFPZdhj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_new = []\n",
        "for i in y_train:\n",
        "  if i == 'business':\n",
        "    y_train_new.append(0)\n",
        "  if i == 'entertainment':\n",
        "    y_train_new.append(1)\n",
        "  if i == 'politics':\n",
        "    y_train_new.append(2)\n",
        "  if i == 'sport':\n",
        "    y_train_new.append(3)\n",
        "  if i == 'tech':\n",
        "    y_train_new.append(4)\n",
        "\n",
        "y_test_new = []\n",
        "for i in y_test:\n",
        "  if i == 'business':\n",
        "    y_test_new.append(0)\n",
        "  if i == 'entertainment':\n",
        "    y_test_new.append(1)\n",
        "  if i == 'politics':\n",
        "    y_test_new.append(2)\n",
        "  if i == 'sport':\n",
        "    y_test_new.append(3)\n",
        "  if i == 'tech':\n",
        "    y_test_new.append(4)\n",
        "  "
      ],
      "metadata": {
        "id": "erTDxRWieWyP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class NGramClassifier:\n",
        "    def __init__(self, ngram_range=(1, 2)):\n",
        "        self.clf = Pipeline([\n",
        "            ('vect', CountVectorizer(ngram_range=ngram_range)),\n",
        "            ('clf', MultinomialNB())\n",
        "        ])\n",
        "        \n",
        "    def train(self, X_train, y_train):\n",
        "        self.clf.fit(X_train, y_train)\n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        return self.clf.predict(X_test)"
      ],
      "metadata": {
        "id": "-c-nWif8fiFV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NGramClassifier()\n",
        "clf.train(X_train_new, y_train_new)\n",
        "y_pred = clf.predict(X_test_new)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyn0pd5hfm6u",
        "outputId": "91286dec-e1f1-4e65-8249-242585c11d21"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 1 2 4 3 0 2 1 4 4 2 3 1 1 4 3 2 4 3 2 2 1 4 2 0 0 3 1 4 3 2 4 3 2 0 0\n",
            " 2 0 4 1 4 3 1 1 1 3 1 1 3 1 3 0 2 0 4 2 2 3 3 4 3 2 2 0 2 4 0 1 0 1 4 3 2\n",
            " 2 3 1 1 3 2 4 0 3 0 0 1 3 2 4 1 3 3 0 2 4 0 1 4 0 3 3 0 0 0 3 2 1 4 0 0 0\n",
            " 3 1 4 2 4 1 2 1 3 4 3 3 3 0 4 3 1 1 3 0 2 2 3 1 1 2 0 2 1 2 0 0 2 3 4 0 1\n",
            " 3 1 0 0 3 1 0 2 0 4 0 1 2 3 2 4 3 0 3 4 3 2 1 1 1 0 3 2 2 0 2 0 4 4 4 2 2\n",
            " 0 1 3 0 0 2 1 0 0 4 2 3 0 4 4 4 1 1 3 2 0 1 0 3 2 2 4 3 4 0 3 3 3 1 2 4 1\n",
            " 3 2 3 2 0 4 3 3 4 0 4 1 2 3 3 4 4 4 4 1 2 1 1 4 1 3 4 0 4 0 1 4 2 0 2 1 0\n",
            " 3 0 3 3 4 3 4 0 1 4 2 4 0 0 0 2 4 1 1 2 0 3 0 3 2 0 3 0 2 1 4 2 3 0 4 2 0\n",
            " 3 2 0 2 3 3 0 2 3 1 0 3 3 1 0 4 2 3 1 2 1 3 4 3 1 3 4 0 3 0 2 0 2 1 1 0 3\n",
            " 2 3 3 3 1 1 1 0 4 3 3 2 2 0 3 0 1 0 2 0 3 2 4 1 3 4 4 0 4 4 0 3 4 0 2 2 3\n",
            " 3 2 1 0 4 2 4 0 0 3 1 4 2 0 4 3 3 3 2 3 4 0 0 3 0 3 0 4 1 3 4 3 3 4 1 4 3\n",
            " 3 3 3 0 1 0 0 4 2 4 0 0 0 4 4 2 1 3 0 1 4 4 2 3 1 4 3 0 0 0 1 1 1 4 0 0 2\n",
            " 2 4 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming y_true is an array of true labels and y_pred is an array of predicted labels\n",
        "accuracy = accuracy_score(y_test_new, y_pred)\n",
        "precision = precision_score(y_test_new, y_pred,average='micro')\n",
        "recall = recall_score(y_test_new, y_pred,average='micro')\n",
        "f1 = f1_score(y_test_new, y_pred,average='micro')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q1n9yR1f3Oa",
        "outputId": "e5e45c2d-d575-4b82-bda1-cd4e510c11b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9821029082774049\n",
            "Precision: 0.9821029082774049\n",
            "Recall: 0.9821029082774049\n",
            "F1 score: 0.9821029082774049\n"
          ]
        }
      ]
    }
  ]
}